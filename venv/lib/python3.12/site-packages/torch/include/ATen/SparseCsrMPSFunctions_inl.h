#pragma once
// @generated by torchgen/gen.py from DispatchKeyFunctions_inl.h

// NB: The implementing C++ file is RegisterDispatchKey.cpp

// The only #includes we need are for custom classes that have defaults in the C++ API
#include <c10/core/MemoryFormat.h>
#include <c10/core/Scalar.h>
#include <ATen/core/Reduction.h>

#if defined(AT_PER_OPERATOR_HEADERS) && defined(TORCH_ASSERT_ONLY_METHOD_OPERATORS)
#error This change adds a dependency on all pytorch operators, meaning the     \
  file will need to be re-compiled every time an operator is changed or added. \
  Consider including a specific operator from                                  \
  <ATen/ops/{my_operator}_sparsecsrmps_dispatch.h>.                   \
  See NOTE [TORCH_ASSERT_ONLY_METHOD_OPERATORS].
#endif

#include <ATen/ops/_conj_physical_sparsecsrmps_dispatch.h>
#include <ATen/ops/_nnz_sparsecsrmps_dispatch.h>
#include <ATen/ops/_to_dense_sparsecsrmps_dispatch.h>
#include <ATen/ops/_to_sparse_sparsecsrmps_dispatch.h>
#include <ATen/ops/abs_sparsecsrmps_dispatch.h>
#include <ATen/ops/conj_physical_sparsecsrmps_dispatch.h>
#include <ATen/ops/dense_dim_sparsecsrmps_dispatch.h>
#include <ATen/ops/sparse_dim_sparsecsrmps_dispatch.h>
#include <ATen/ops/trunc_sparsecsrmps_dispatch.h>



